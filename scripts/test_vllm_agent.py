"""
vLLM ì—°ë™ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ì‹¤ì œ GPUì—ì„œ DeepSeek-R1-Distill ëª¨ë¸ ì‚¬ìš©
"""

import sys
sys.path.insert(0, "/workspace/dev")

import asyncio
import httpx
from src.config import settings


async def test_vllm_direct():
    """vLLM ì§ì ‘ í˜¸ì¶œ í…ŒìŠ¤íŠ¸"""
    print("=" * 60)
    print("vLLM ì§ì ‘ í˜¸ì¶œ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    print(f"ëª¨ë¸: {settings.vllm_model}")
    print(f"ì„œë²„: {settings.vllm_base_url}")
    print()

    async with httpx.AsyncClient(timeout=120.0) as client:
        # ì˜ë£Œ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
        questions = [
            "ì²œì‹ í™˜ìì˜ ê¸‰ì„± ë°œì‘ ì‹œ ì‘ê¸‰ ì²˜ì¹˜ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
            "COPD í™˜ìì—ê²Œ ê¶Œì¥ë˜ëŠ” í¡ì…ê¸° ì¢…ë¥˜ëŠ”?",
            "íë ´ í™˜ìì˜ ì…ì› ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        ]

        for i, question in enumerate(questions, 1):
            print(f"\n{'='*60}")
            print(f"ì§ˆë¬¸ {i}: {question}")
            print("-" * 60)

            response = await client.post(
                f"{settings.vllm_base_url}/v1/chat/completions",
                json={
                    "model": settings.vllm_model,
                    "messages": [
                        {
                            "role": "system",
                            "content": "ë‹¹ì‹ ì€ í˜¸í¡ê¸°ë‚´ê³¼ ì „ë¬¸ì˜ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ì „ë¬¸ì ì¸ ì˜ë£Œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”."
                        },
                        {"role": "user", "content": question}
                    ],
                    "max_tokens": 1024,
                    "temperature": 0.7,
                },
            )

            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]

                # <think> íƒœê·¸ ì²˜ë¦¬ (DeepSeek-R1 íŠ¹ì„±)
                if "</think>" in content:
                    # ì¶”ë¡  ê³¼ì •ê³¼ ìµœì¢… ë‹µë³€ ë¶„ë¦¬
                    parts = content.split("</think>")
                    thinking = parts[0].replace("<think>", "").strip() if "<think>" in parts[0] else ""
                    answer = parts[1].strip() if len(parts) > 1 else content

                    print(f"\nğŸ“ ì¶”ë¡  ê³¼ì • (ìš”ì•½):")
                    # ì¶”ë¡  ê³¼ì • ì²« 200ìë§Œ í‘œì‹œ
                    if thinking:
                        print(f"   {thinking[:200]}..." if len(thinking) > 200 else f"   {thinking}")

                    print(f"\nğŸ’¡ ë‹µë³€:")
                    print(f"   {answer}")
                else:
                    print(f"\nğŸ’¡ ë‹µë³€:")
                    print(f"   {content}")

                # í† í° ì‚¬ìš©ëŸ‰
                usage = result.get("usage", {})
                print(f"\nğŸ“Š í† í°: ì…ë ¥ {usage.get('prompt_tokens', 0)}, ì¶œë ¥ {usage.get('completion_tokens', 0)}")
            else:
                print(f"âŒ ì˜¤ë¥˜: {response.status_code}")
                print(response.text)


async def test_medical_context():
    """ì˜ë£Œ ì»¨í…ìŠ¤íŠ¸ í¬í•¨ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("ì˜ë£Œ ì»¨í…ìŠ¤íŠ¸ í¬í•¨ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # í™˜ì ì •ë³´ì™€ í•¨ê»˜ ì§ˆë¬¸
    patient_context = """
í™˜ì ì •ë³´:
- ë‚˜ì´: 65ì„¸ ë‚¨ì„±
- ì§„ë‹¨: COPD Stage III, ê³ í˜ˆì••
- í˜„ì¬ ì•½ë¬¼: Tiotropium 18mcg 1ì¼ 1íšŒ, Amlodipine 5mg 1ì¼ 1íšŒ
- ìµœê·¼ ì¦ìƒ: í˜¸í¡ê³¤ë€ ì•…í™”, ê°ë‹´ ì¦ê°€ (í™©ìƒ‰)
"""

    question = "ì´ í™˜ìì—ê²Œ ì¶”ê°€ë¡œ í•„ìš”í•œ ì¹˜ë£ŒëŠ” ë¬´ì—‡ì¸ê°€ìš”?"

    async with httpx.AsyncClient(timeout=120.0) as client:
        response = await client.post(
            f"{settings.vllm_base_url}/v1/chat/completions",
            json={
                "model": settings.vllm_model,
                "messages": [
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ í˜¸í¡ê¸°ë‚´ê³¼ ì „ë¬¸ì˜ì…ë‹ˆë‹¤. í™˜ì ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹˜ë£Œ ê³„íšì„ ì œì•ˆí•˜ì„¸ìš”. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”."
                    },
                    {"role": "user", "content": f"{patient_context}\n\nì§ˆë¬¸: {question}"}
                ],
                "max_tokens": 1024,
                "temperature": 0.7,
            },
        )

        if response.status_code == 200:
            result = response.json()
            content = result["choices"][0]["message"]["content"]

            # <think> íƒœê·¸ ì²˜ë¦¬
            if "</think>" in content:
                parts = content.split("</think>")
                answer = parts[1].strip() if len(parts) > 1 else content
            else:
                answer = content

            print(f"\ní™˜ì ì •ë³´:")
            print(patient_context)
            print(f"\nì§ˆë¬¸: {question}")
            print(f"\nğŸ’¡ ì˜ì‚¬ ë‹µë³€:")
            print(answer)
        else:
            print(f"âŒ ì˜¤ë¥˜: {response.status_code}")


async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸"""
    # vLLM ì„œë²„ ìƒíƒœ í™•ì¸
    async with httpx.AsyncClient() as client:
        try:
            response = await client.get(f"{settings.vllm_base_url}/v1/models")
            if response.status_code != 200:
                print("âŒ vLLM ì„œë²„ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return
        except Exception as e:
            print(f"âŒ vLLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            return

    print("âœ… vLLM ì„œë²„ ì—°ê²° ì„±ê³µ!\n")

    await test_vllm_direct()
    await test_medical_context()

    print("\n" + "=" * 60)
    print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
