"""
GPU Task Manager Script
GPU ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ë° ì‘ì—… ê´€ë¦¬
"""

import os
import json
import argparse
import subprocess
from pathlib import Path
from typing import Dict, List, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import time


@dataclass
class GPUStatus:
    """GPU ìƒíƒœ ì •ë³´"""
    index: int
    name: str
    memory_used_gb: float
    memory_total_gb: float
    memory_percent: float
    temperature: int
    utilization: int
    power_draw: int
    power_limit: int


@dataclass
class TrainingTask:
    """í•™ìŠµ ì‘ì—… ì •ë³´"""
    task_id: str
    script: str
    gpu_id: int
    status: str  # queued, running, completed, failed
    submitted_at: str
    started_at: Optional[str] = None
    completed_at: Optional[str] = None
    pid: Optional[int] = None


class GPUMonitor:
    """GPU ëª¨ë‹ˆí„°ë§"""

    def __init__(self):
        self._check_nvidia_smi()

    def _check_nvidia_smi(self):
        """nvidia-smi ì‚¬ìš© ê°€ëŠ¥ í™•ì¸"""
        try:
            subprocess.run(['nvidia-smi'], capture_output=True, check=True)
        except (subprocess.CalledProcessError, FileNotFoundError):
            raise RuntimeError("nvidia-smië¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. NVIDIA ë“œë¼ì´ë²„ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")

    def get_gpu_status(self) -> List[GPUStatus]:
        """ëª¨ë“  GPU ìƒíƒœ ì¡°íšŒ"""
        try:
            import pynvml
            pynvml.nvmlInit()

            gpus = []
            device_count = pynvml.nvmlDeviceGetCount()

            for i in range(device_count):
                handle = pynvml.nvmlDeviceGetHandleByIndex(i)

                name = pynvml.nvmlDeviceGetName(handle)
                if isinstance(name, bytes):
                    name = name.decode('utf-8')

                memory = pynvml.nvmlDeviceGetMemoryInfo(handle)
                temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
                util = pynvml.nvmlDeviceGetUtilizationRates(handle)
                power = pynvml.nvmlDeviceGetPowerUsage(handle) // 1000
                power_limit = pynvml.nvmlDeviceGetPowerManagementLimit(handle) // 1000

                gpus.append(GPUStatus(
                    index=i,
                    name=name,
                    memory_used_gb=memory.used / (1024**3),
                    memory_total_gb=memory.total / (1024**3),
                    memory_percent=(memory.used / memory.total) * 100,
                    temperature=temp,
                    utilization=util.gpu,
                    power_draw=power,
                    power_limit=power_limit,
                ))

            pynvml.nvmlShutdown()
            return gpus

        except ImportError:
            # pynvmlì´ ì—†ìœ¼ë©´ nvidia-smi íŒŒì‹±
            return self._parse_nvidia_smi()

    def _parse_nvidia_smi(self) -> List[GPUStatus]:
        """nvidia-smi ì¶œë ¥ íŒŒì‹±"""
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=index,name,memory.used,memory.total,temperature.gpu,utilization.gpu,power.draw,power.limit',
             '--format=csv,noheader,nounits'],
            capture_output=True,
            text=True
        )

        gpus = []
        for line in result.stdout.strip().split('\n'):
            parts = [p.strip() for p in line.split(',')]
            if len(parts) >= 8:
                mem_used = float(parts[2]) / 1024  # MB to GB
                mem_total = float(parts[3]) / 1024

                gpus.append(GPUStatus(
                    index=int(parts[0]),
                    name=parts[1],
                    memory_used_gb=mem_used,
                    memory_total_gb=mem_total,
                    memory_percent=(mem_used / mem_total) * 100,
                    temperature=int(parts[4]),
                    utilization=int(parts[5]),
                    power_draw=int(float(parts[6])),
                    power_limit=int(float(parts[7])),
                ))

        return gpus

    def print_status(self):
        """GPU ìƒíƒœ ì¶œë ¥"""
        gpus = self.get_gpu_status()

        print("=== GPU ìƒíƒœ ===\n")
        for gpu in gpus:
            print(f"GPU {gpu.index}: {gpu.name}")
            print(f"  - ë©”ëª¨ë¦¬: {gpu.memory_used_gb:.1f} / {gpu.memory_total_gb:.1f} GB ({gpu.memory_percent:.1f}%)")
            print(f"  - ì˜¨ë„: {gpu.temperature}Â°C")
            print(f"  - ì‚¬ìš©ë¥ : {gpu.utilization}%")
            print(f"  - ì „ë ¥: {gpu.power_draw}W / {gpu.power_limit}W")
            print()

    def find_available_gpu(self, min_memory_gb: float = 10.0) -> Optional[int]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ GPU ì°¾ê¸°"""
        gpus = self.get_gpu_status()

        for gpu in gpus:
            available_gb = gpu.memory_total_gb - gpu.memory_used_gb
            if available_gb >= min_memory_gb and gpu.utilization < 50:
                return gpu.index

        return None


class BatchSizeOptimizer:
    """ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""

    # ëª¨ë¸ë³„ íŒŒë¼ë¯¸í„° ìˆ˜ (approximate)
    MODEL_PARAMS = {
        '1B': 1e9,
        '3B': 3e9,
        '7B': 7e9,
        '13B': 13e9,
        '32B': 32e9,
        '70B': 70e9,
    }

    def calculate_memory_per_sample(
        self,
        model_size: str,
        seq_length: int,
        dtype: str = 'bf16',
        lora_rank: int = 32,
    ) -> float:
        """ìƒ˜í”Œë‹¹ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • (GB)"""
        params = self.MODEL_PARAMS.get(model_size, 7e9)

        # ë°”ì´íŠ¸ í¬ê¸°
        dtype_bytes = 2 if dtype in ['bf16', 'fp16'] else 4

        # ëª¨ë¸ ë©”ëª¨ë¦¬ (ì–‘ìí™” ê³ ë ¤ ì•ˆí•¨)
        model_memory = params * dtype_bytes / (1024**3)

        # LoRA ì¶”ê°€ ë©”ëª¨ë¦¬
        lora_memory = params * 0.01 * lora_rank / 32 * dtype_bytes / (1024**3)

        # í™œì„±í™” ë©”ëª¨ë¦¬ (ëŒ€ëµì  ì¶”ì •)
        hidden_size = int((params / 1e9) ** 0.5 * 4096)  # ê·¼ì‚¬ì¹˜
        activation_memory = seq_length * hidden_size * dtype_bytes * 4 / (1024**3)

        # ê·¸ë˜ë””ì–¸íŠ¸ ë©”ëª¨ë¦¬
        gradient_memory = activation_memory * 0.5

        return model_memory + lora_memory + activation_memory + gradient_memory

    def suggest_batch_size(
        self,
        available_memory_gb: float,
        model_size: str,
        seq_length: int = 2048,
        dtype: str = 'bf16',
        lora_rank: int = 32,
    ) -> int:
        """ìµœì  ë°°ì¹˜ í¬ê¸° ì œì•ˆ"""
        memory_per_sample = self.calculate_memory_per_sample(
            model_size, seq_length, dtype, lora_rank
        )

        # ì•ˆì „ ë§ˆì§„ 20%
        usable_memory = available_memory_gb * 0.8

        batch_size = int(usable_memory / memory_per_sample)
        return max(1, batch_size)


class TaskQueue:
    """ì‘ì—… í ê´€ë¦¬"""

    def __init__(self, queue_file: str = ".gpu_task_queue.json"):
        self.queue_file = Path(queue_file)
        self.tasks: List[TrainingTask] = []
        self._load_queue()

    def _load_queue(self):
        """í íŒŒì¼ ë¡œë“œ"""
        if self.queue_file.exists():
            data = json.loads(self.queue_file.read_text())
            self.tasks = [TrainingTask(**t) for t in data]

    def _save_queue(self):
        """í íŒŒì¼ ì €ì¥"""
        data = [asdict(t) for t in self.tasks]
        self.queue_file.write_text(json.dumps(data, ensure_ascii=False, indent=2))

    def submit(self, script: str, gpu_id: int = 0) -> TrainingTask:
        """ì‘ì—… ì œì¶œ"""
        task = TrainingTask(
            task_id=f"task_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            script=script,
            gpu_id=gpu_id,
            status="queued",
            submitted_at=datetime.now().isoformat(),
        )
        self.tasks.append(task)
        self._save_queue()
        return task

    def list_tasks(self):
        """ì‘ì—… ëª©ë¡ ì¶œë ¥"""
        print("=== ì‘ì—… í ===\n")
        for task in self.tasks:
            status_icon = {
                'queued': 'â³',
                'running': 'ğŸ”„',
                'completed': 'âœ…',
                'failed': 'âŒ',
            }.get(task.status, '?')

            print(f"{status_icon} [{task.task_id}] {task.script}")
            print(f"   GPU: {task.gpu_id}, ìƒíƒœ: {task.status}")
            print(f"   ì œì¶œ: {task.submitted_at}")
            print()


def main():
    parser = argparse.ArgumentParser(description='GPU Task Manager')
    subparsers = parser.add_subparsers(dest='command', help='Commands')

    # status
    subparsers.add_parser('status', help='GPU ìƒíƒœ í™•ì¸')

    # submit
    submit_parser = subparsers.add_parser('submit', help='ì‘ì—… ì œì¶œ')
    submit_parser.add_argument('--script', required=True, help='í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸')
    submit_parser.add_argument('--gpu', type=int, default=0, help='GPU ID')

    # queue
    subparsers.add_parser('queue', help='ì‘ì—… í í™•ì¸')

    # optimal-batch
    batch_parser = subparsers.add_parser('optimal-batch', help='ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°')
    batch_parser.add_argument('--model', required=True, help='ëª¨ë¸ í¬ê¸° (1B, 7B, 13B, 32B, 70B)')
    batch_parser.add_argument('--seq-length', type=int, default=2048)
    batch_parser.add_argument('--gpu', type=int, default=0, help='GPU ID')

    args = parser.parse_args()

    if args.command == 'status':
        monitor = GPUMonitor()
        monitor.print_status()

    elif args.command == 'submit':
        queue = TaskQueue()
        task = queue.submit(args.script, args.gpu)
        print(f"ì‘ì—… ì œì¶œë¨: {task.task_id}")

    elif args.command == 'queue':
        queue = TaskQueue()
        queue.list_tasks()

    elif args.command == 'optimal-batch':
        monitor = GPUMonitor()
        optimizer = BatchSizeOptimizer()

        gpus = monitor.get_gpu_status()
        if args.gpu < len(gpus):
            gpu = gpus[args.gpu]
            available = gpu.memory_total_gb - gpu.memory_used_gb

            batch_size = optimizer.suggest_batch_size(
                available_memory_gb=available,
                model_size=args.model,
                seq_length=args.seq_length,
            )

            print(f"=== ìµœì  ë°°ì¹˜ í¬ê¸° ===")
            print(f"GPU {args.gpu}: {gpu.name}")
            print(f"ê°€ìš© ë©”ëª¨ë¦¬: {available:.1f} GB")
            print(f"ëª¨ë¸: {args.model}")
            print(f"ì‹œí€€ìŠ¤ ê¸¸ì´: {args.seq_length}")
            print(f"ì¶”ì²œ ë°°ì¹˜ í¬ê¸°: {batch_size}")
        else:
            print(f"GPU {args.gpu}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    else:
        parser.print_help()


if __name__ == '__main__':
    main()
